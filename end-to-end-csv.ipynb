{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()  # this could also be a hard-coded bucket name\n",
    "\n",
    "prefix = \"nextera/monitoring\"\n",
    "\n",
    "print(\"Using bucket \" + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5ff3",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "df=pd.read_csv(\"data/X.csv\")\n",
    "\n",
    "df['Categorical'] = encoder.fit_transform(df['Categorical'])\n",
    "X = df[list(df)[:-1]]\n",
    "Y = df[list(df)[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=list(X))\n",
    "trainX[\"Categorical\"] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=list(X))\n",
    "testX[\"Categorical\"] = y_test\n",
    "\n",
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.to_csv(\"X_train.csv\", index=False)\n",
    "testX.to_csv(\"X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cf9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to S3. SageMaker will take training data from s3\n",
    "trainpath = sess.upload_data(\n",
    "    path=\"X_train.csv\", bucket=bucket, key_prefix=prefix\n",
    ")\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path=\"X_test.csv\", bucket=bucket, key_prefix=prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df393064",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# inference functions ---------------\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"X_train.csv\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"X_test.csv\")\n",
    "    parser.add_argument(\n",
    "        \"--features\", type=str\n",
    "    )  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str\n",
    "    )  # in this script we ask user to explicitly name the target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"reading data\")\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print(\"building training and testing datasets\")\n",
    "    \n",
    "    X_train = train_df[args.features.split()]\n",
    "    X_test = test_df[args.features.split()]\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "\n",
    "    # train\n",
    "    print(\"training model\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators, min_samples_leaf=args.min_samples_leaf, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # print abs error\n",
    "    print(\"validating model\")\n",
    "    abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "\n",
    "    # print couple perf metrics\n",
    "    for q in [10, 50, 90]:\n",
    "        print(\"AE-at-\" + str(q) + \"th-percentile: \" + str(np.percentile(a=abs_err, q=q)))\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print(\"model persisted at \" + path)\n",
    "    print(args.min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(input_list):\n",
    "    string = ''\n",
    "    for item in input_list:\n",
    "        string += f\"{item} \"\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "features = list_to_string(list(df)[:-1])\n",
    "\n",
    "target = list(df)[-1]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python script.py --n-estimators 100 \\\n",
    "                   --min-samples-leaf 2 \\\n",
    "                   --model-dir ./ \\\n",
    "                   --train ./ \\\n",
    "                   --test ./ \\\n",
    "                   --features 'Feature_1 Feature_2 Feature_3 Feature_4 Feature_5' \\\n",
    "                   --target {target}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e16dd2",
   "metadata": {},
   "source": [
    "### SageMaker Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"rf-scikit\",\n",
    "    metric_definitions=[{\"Name\": \"median-AE\", \"Regex\": \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n",
    "    hyperparameters={\n",
    "        \"n-estimators\": 100,\n",
    "        \"min-samples-leaf\": 3,\n",
    "        \"features\": features,\n",
    "        \"target\": target,\n",
    "    },\n",
    ")\n",
    "\n",
    "# launch training job, with asynchronous call\n",
    "sklearn_estimator.fit({\"train\": trainpath, \"test\": testpath}, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ecd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator.latest_training_job.wait(logs=\"None\")\n",
    "artifact = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=sklearn_estimator.latest_training_job.name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(\"Model artifact persisted at \" + artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8344828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "model = SKLearnModel(\n",
    "    model_data=artifact,\n",
    "    role=get_execution_role(),\n",
    "    entry_point=\"inference_handler.py\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(prefix.replace('/', '-'))\n",
    "\n",
    "s3_capture_upload_path = f\"s3://{bucket}/{prefix}/data-capture\"\n",
    "\n",
    "# Specify either Input, Output or both. \n",
    "capture_modes = ['REQUEST','RESPONSE']\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture = True, \n",
    "    sampling_percentage = 100, # Optional\n",
    "    destination_s3_uri = s3_capture_upload_path, # Optional\n",
    "    capture_options = [\"REQUEST\", \"RESPONSE\"],\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    instance_type=\"ml.c5.large\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0a3ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# authenticating with AWS\n",
    "runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "\n",
    "data = testX[list(df)[:-1]]\n",
    "\n",
    "csv_file = io.StringIO()\n",
    "\n",
    "#loop over each row of pandas df and convert each row to json\n",
    "for index, row in data.iterrows():\n",
    "    row = row.to_list()\n",
    "    payload = \",\".join(str(x) for x in row)\n",
    "\n",
    "    # invoking endpoint\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=payload,\n",
    "        Accept=\"text/csv\",\n",
    "        ContentType=\"text/csv\",  # for csv 'application/x-npy' for numpy\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response[\"Body\"].read())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430cc2f1",
   "metadata": {},
   "source": [
    "### View captured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# the data capture may take a few seconds to appear\n",
    "time.sleep(60)\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = f\"{prefix}/data-capture/{endpoint_name}\"\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e69b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e8bd4",
   "metadata": {},
   "source": [
    "### Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cf303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from monitoringjob_utils import run_model_monitor_job_processor\n",
    "\n",
    "#Create a monitoring object\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results_uri = f\"s3://{bucket}/{prefix}/baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eda201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start baseline job\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=\"X_train.csv\",\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f9c5d",
   "metadata": {},
   "source": [
    "### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=f\"{prefix}/baseline\")\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d7d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8dce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf47af",
   "metadata": {},
   "source": [
    "### Trigger job instantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ca69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_stats = ''\n",
    "s3_const = \"\"\n",
    "\n",
    "for file in report_files:\n",
    "    if \"statistics\" in file:\n",
    "        s3_stats=f\"s3://{bucket}/{file}\"\n",
    "    else:\n",
    "        s3_const=f\"s3://{bucket}/{file}\"\n",
    "print(s3_stats)\n",
    "print(s3_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_prefix = f\"{prefix}/preprocess\"\n",
    "preprocess_file = \"preprocess_v8.py\"\n",
    "\n",
    "trainpath = sess.upload_data(\n",
    "    path=preprocess_file, bucket=bucket, key_prefix=preprocess_prefix\n",
    ")\n",
    "\n",
    "preprocess_path = f\"s3://{bucket}/{preprocess_prefix}/{preprocess_file}\"\n",
    "print(preprocess_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739eb825",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_path = name_from_base('reports')\n",
    "\n",
    "\n",
    "processor = run_model_monitor_job_processor(\n",
    "    region = region,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    role = get_execution_role(),\n",
    "    data_capture_path = f\"{s3_capture_upload_path}/{endpoint_name}\",\n",
    "    statistics_path = s3_stats,\n",
    "    constraints_path = s3_const,\n",
    "    reports_path = f\"s3://{bucket}/{prefix}/{reports_path}\",\n",
    "#     preprocessor_path=preprocess_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77237787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_monitor_processing_job_name(base_job_name):\n",
    "    client = boto3.client(\"sagemaker\")\n",
    "    response = client.list_processing_jobs(\n",
    "        NameContains=base_job_name,\n",
    "        SortBy=\"CreationTime\",\n",
    "        SortOrder=\"Descending\",\n",
    "        StatusEquals=\"Completed\",\n",
    "    )\n",
    "    if len(response[\"ProcessingJobSummaries\"]) > 0:\n",
    "        return response[\"ProcessingJobSummaries\"][0][\"ProcessingJobName\"]\n",
    "    else:\n",
    "        raise Exception(\"Processing job not found.\")\n",
    "\n",
    "\n",
    "def get_model_monitor_processing_job_s3_report(job_name):\n",
    "    client = boto3.client(\"sagemaker\")\n",
    "    response = client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    s3_report_path = response[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    return s3_report_path\n",
    "\n",
    "\n",
    "MODEL_MONITOR_JOB_NAME = \"sagemaker-model-monitor-analyzer\"\n",
    "latest_model_monitor_processing_job_name = get_latest_model_monitor_processing_job_name(\n",
    "    MODEL_MONITOR_JOB_NAME\n",
    ")\n",
    "print(latest_model_monitor_processing_job_name)\n",
    "report_path = get_model_monitor_processing_job_s3_report(latest_model_monitor_processing_job_name)\n",
    "print(report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket, Prefix=f\"{prefix}/{reports_path}\")\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_file = get_obj_body(report_files[0])\n",
    "print(capture_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9eddf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2816e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
